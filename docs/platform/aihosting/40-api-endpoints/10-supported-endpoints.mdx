---
sidebar_label: Supported endpoints
description: Supported API endpoints and usage examples
title: Supported API endpoints
---

The API is accessible via HTTPS and is compatible with the OpenAI API. The base URL is `https://llm.aihosting.mittwald.de`. Many applications require specifying the version as well. In this case, the full base URL with versioning should be used: `https://llm.aihosting.mittwald.de/v1`.

Every interaction with the API requires an `Authorization` header with a valid API key. This key can be created in mStudio.

In our examples, we use `curl`, as it is the simplest and quickest way to test. For production use, we recommend using frameworks and libraries that support OpenAI. To run the following examples please setup your API key as an environemt variable first, e.g.

```sh
export APIKEY=sk-â€¦
```

## /v1/models

This endpoint returns a list of available models.

```sh
curl -i -X GET https://llm.aihosting.mittwald.de/v1/models \
    -H "Content-Type: application/json" \
    -H "Authorization: Bearer $APIKEY"
```

It returns a dictionary containing a list of available models. The `id` within this list can be used in the `model` field for subsequent API routes.

## /v1/chat/completions and /v1/completions

This route allows content to be sent to the LLM in chat format. The endpoint supports streaming for the content generated by the LLM.

```sh
curl -i -X POST https://llm.aihosting.mittwald.de/v1/chat/completions \
    -H "Content-Type: application/json" \
    -H "Authorization: Bearer $APIKEY" \
    -d '{ 
        "model": "Mistral-Small-3.2-24B-Instruct",
        "messages": [
            {
                "role": "user",
                "content": "Moin and hello!"
            }
        ]
    }'

```

The `model` parameter requires a valid model name, which can be retrieved via the `/v1/models` route. Additional model parameters such as `temperature`, `top_p`, or `top_k` can be provided. Recommended settings for these can be found in the model descriptions if they differ from the defaults. Which extended parameters influence the response depends on the model.

To receive a streamed response, the option `stream: true` must be set.

```sh
curl -i -N -X POST https://llm.aihosting.mittwald.de/v1/chat/completions \
    -H "Content-Type: application/json" \
    -H "Authorization: Bearer $APIKEY" \
    -d '{ 
        "model": "Mistral-Small-3.2-24B-Instruct",
        "messages": [
            {
                "role": "user",
                "content": "Moin and hello!"
            }
        ],
        "stream": true,
        "temperature": 0.15,
        "top_k": 10,
        "top_p": 0.5
    }'
```

## /v1/embeddings

This route allows you to generate embeddings for texts.

```sh
curl -i -X POST https://llm.aihosting.mittwald.de/v1/embeddings \
    -H "Content-Type: application/json" \
    -H "Authorization: Bearer $APIKEY" \
    -d '{ 
        "model": "Qwen3-Embedding-8B",
        "input": "An important document"
    }'
```

Depending on the embedding model used, additional supported parameters such as `dimensions` can be submitted. However, this is not applicable to [Qwen3-Embedding-8B](../../models/qwen3-embedding-8b/).

